[LOG] Writing training logs to checkpoints_dpdd/lr0.0001_r4_doc3_th0.5_yrpre-2018_rep2/train.log

Epoch 1/2
[DPDD] Score after epoch 1: 11.2439
[DPDD] Dropped 20 / 202 samples for next epoch.
[Checkpoint] Model saved to checkpoints_dpdd/lr0.0001_r4_doc3_th0.5_yrpre-2018_rep2/checkpoint_epoch_1
[Validation] Loss after epoch 1: 0.9648

Epoch 2/2
[DPDD] Score after epoch 2: 10.5498
[DPDD] Dropped 18 / 182 samples for next epoch.
[Checkpoint] Model saved to checkpoints_dpdd/lr0.0001_r4_doc3_th0.5_yrpre-2018_rep2/checkpoint_epoch_2
[Validation] Loss after epoch 2: 0.8062
[HF Save] Model and tokenizer saved to checkpoints_dpdd/lr0.0001_r4_doc3_th0.5_yrpre-2018_rep2/hf_format
Training complete.
ðŸ” Generating summaries...
/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:418: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
[âœ“] Saved generated summaries to checkpoints_dpdd/lr0.0001_r4_doc3_th0.5_yrpre-2018_rep2/generated_summaries.jsonl
[LOG] Writing training logs to checkpoints_dpdd/lr0.0001_r4_doc3_th0.5_yrpre-2018_rep2/train.log

Epoch 1/2
[DPDD] Score after epoch 1: 10.5277
[DPDD] Dropped 20 / 202 samples for next epoch.
[Checkpoint] Model saved to checkpoints_dpdd/lr0.0001_r4_doc3_th0.5_yrpre-2018_rep2/checkpoint_epoch_1
[Validation] Loss after epoch 1: 0.9623

Epoch 2/2
[DPDD] Score after epoch 2: 11.1001
[DPDD] Dropped 18 / 182 samples for next epoch.
[Checkpoint] Model saved to checkpoints_dpdd/lr0.0001_r4_doc3_th0.5_yrpre-2018_rep2/checkpoint_epoch_2
[Validation] Loss after epoch 2: 0.8003
[HF Save] Model and tokenizer saved to checkpoints_dpdd/lr0.0001_r4_doc3_th0.5_yrpre-2018_rep2/hf_format
Training complete.
ðŸ” Generating summaries...
/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )
  warnings.warn(
/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:418: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(
[âœ“] Saved generated summaries to checkpoints_dpdd/lr0.0001_r4_doc3_th0.5_yrpre-2018_rep2/generated_summaries.jsonl
